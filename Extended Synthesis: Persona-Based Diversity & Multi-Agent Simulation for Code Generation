Warning: 1) Andrej Karpathy has nothing to do with this research, except for him involuntarily inspiring it through comments in a thread on X (see below). 2) This is synthesized research from 11 frontier AI models, including GPT 5.1 Pro and Gemini 3 Deep Think, requiring Pro and Ultra subscriptions respectively. If you are (understably) not interested in reading AI output, stop reading now. But I believe this may be worth your time.
Title: Extended Synthesis: Persona-Based Diversity & Multi-Agent Simulation for Code Generation
Two Architectural Paradigms for N-Completion Workflows
Abstract: (Written by ZAI AI GLM 4.6 Deep Think + Web):
This research report investigates methodologies for enhancing AI-generated code quality by leveraging persona diversity and multi-agent architectures. Synthesizing outputs from 11 advanced AI models, the study explores two primary paradigms. The first, the "Diversity Council," utilizes parallel generation with multiple persona prompts to maximize solution diversity and increase the probability of discovering an outlier, high-quality solution (Pass@K). Key findings indicate that simulation-based persona framing ("What would X code?") is more effective than direct role-playing, and that diversity-aware selection outperforms consensus-based approaches. The second paradigm, the "Simulated Engineering Department," employs iterative refinement through a small team of specialized agents, demonstrating that minimal, focused teams outperform larger ones, particularly when code generation and test design are strictly separated to prevent bias. The report concludes that while both architectures are effective, a hybrid approach combining initial parallel diversity with subsequent iterative refinement may be optimal. Crucially, the research identifies that the primary challenge in production environments is not model capability but system design, with a significant portion of failures stemming from inter-agent misalignment, underscoring the need for robust architectural principles over advanced prompting alone.
Methodology (Written by Scott Novak)
The same query was run on the following 11 Frontier AI models (14 times total runs due to Gemini 3 Deep Think being utilized 4 times) run via chat interface. Claude Opus 4.5 Research was used to adjudicate and synthesize the best parts of each of these query reports. Anytime web access, thinking options and research/deep research options were available, these options were enabled:
ChatGPT 5.1 Pro 
ChatGPT Deep Research
Gemini Deep Research
Gemini 3 Deep Think 4X (ie query was ran 4 times and all were incorporated)
Claude Opus 4.5 Research
Perplexity Pro Deep Research
Qwen 3 Max with Deep Think (Maximum thinking tokens)
Minimax M2 (research agent)
DeepSeek v3.2
Zai AI GLM 4.6 Deep Think
ErnieBot 4.5 Turbo

Query: 
“Please research these possibilities: Many agentic coding setups use multiple different AIs (including even multiple single instances of the same AI) with slightly different prompts (ie do it basic and secure, do it creatively, etc.) To create n code completions. It is known that one shot AI persona prompts do not generally increase average quality of responses. But for n completion workflows, diversity is more important so outlier solutions can be evaluated. Is it possible to increase diversity of solutions with *multiple* persona prompts paired with model diversity. Either direct persona "you are John Carmack" (repeated n times with other prominent and/or prolific programmers with examples available on web and or llm corpus) type prompts or Andrej Karpathy style "what code would John Carmack code" (repeated n times as well). There is also the idea that, though one shot persona prompts may not increase response quality *on average*, iterated exchanges moving an LLM into a basin (see usually safe LLMs encouraging psychosis in users in long conversations as they are gradually moved to a new basin) may be possible for useful things (science, business, code generation which is what I am interested here). 
I was inspired by this exchange on X: 
Andrej Karpathy (@karpathy) Dec 7th: Don't think of LLMs as entities but as simulators. For example, when exploring a topic, don't ask: "What do you think about xyz?" There is no "you". Next time try: "What would be a good group of people to explore xyz? What would they say?" The LLM can channel/simulate many perspectives, but it hasn't "thought about" xyz for a while and over time and formed its own opinions in the way we're used to. If you force it via the use of "you", it will give you something by adopting a personality embedding vector implied by the statistics of its finetuning data and then simulate that. It's fine to do, but there is a lot less mystique to it than I find people naively attribute to "asking an AI". 
Dimitris Papailiopoulos (@DimitrisPapail) Dec 7: Wouldn’t it adopt the personality that would be expected to be of highest reward, which would therefore be that of the expert of the area most suited to answer the question? 
Andrej Karpathy (@karpathy) Dec 7: There is definitely work going into engineering the "you" simulation—the personality that gets all the rewards in verifiable problems, or all the upvotes from users/judge LLMs, or mimics the responses of SFT—and there is an emergent composite personality from that. My point is more that the "you" there is deliberately bolted on, engineered and layered on what is fundamentally a token simulation engine, not a mind that is somehow emergent and over time constructed in a relatable way to an average person talking to an AI. The story is a bit more simple in verifiable domains, but I think more interesting/complicated in the non-verifiable ones, such as for example asking about opinions on topics xyz. It's less clear how to think about the "you" that you're talking to or where it comes from and what credence you should give it. 
Christian Szegedy (@ChrSzegedy), Dec 8: I think even this is subtly wrong. Your prompt—"what does a certain group of people think"—elicits a simulation about what a certain kind of fictional entity (described in the system prompt) would think about the opinion of that group. 
Andrej Karpathy (@karpathy), Dec 8: I could certainly imagine that "nesting" the simulation might be too "effortful" for the model, compute or data density wise. My results with it are not too bad so in my opinion it's at least worth people trying, experimenting with, and thinking about. For example, it might be useful to read multiple distinct and approximate perspectives on topic xyz instead of one. Research-wise, you might be able to elicit LLM Council-like benefits (not via diverse LLMs but via diverse simulations), and improve performance via the generator-discriminator gap effects or ensembling effects”.
Executive Summary
This synthesis presents two complementary architectural paradigms for maximizing code quality through AI-assisted generation: the Diversity Council (parallel generation with persona-based diversity) and the Simulated Engineering Department (iterative multi-agent collaboration over extended turns). Both approaches are validated by research, but serve different use cases and optimization targets.
Key finding: Three specialized agents (programmer, tester, executor) achieve 96.3% on HumanEval—outperforming both single-agent baselines and larger multi-agent teams. However, production deployments show 36.9% coordination failure rates, indicating that architectural design matters more than model capability.
 
PART I: The Diversity Council Architecture
Parallel generation with persona-based diversity for outlier discovery
Core Thesis
Personas are diversity knobs, not quality boosters. One-shot persona prompts do not improve average response quality. However, for n-completion workflows where you generate N solutions and select the best, average quality is irrelevant. What matters is Pass@K—the probability that at least one solution in your set is correct or exceptional.
Quantified scaling effect: OpenAI's Codex achieved 28.8% success on HumanEval with one try, but 70.2% with 100 tries. This 2.4× improvement comes purely from variance. Personas increase semantic variance (different algorithms) beyond what temperature alone provides (lexical variance).
Consolidated Evidence Base
Finding	Source	Magnitude
Multiple samples increase success	Codex study	28.8% → 70.2%
Multi-persona self-collaboration (SPP)	GPT-4 study	+23% over CoT
Multi-model ensemble vs best single	HumanEval (2025)	90.2% vs 83.5%
Diversity vs consensus selection	Ensemble study	80% vs 47% potential
Jekyll & Hyde ensemble	arXiv 2024	+9.98% average
The Two Framings: Entity vs. Simulator
Identity (Role-Play): "You are John Carmack. Write this function."
•	Hits RLHF "personality vectors"
•	Produces caricatures: mimics tone/style but defaults to average competence
•	Triggers "helpful assistant" safety constraints
Simulation (Prediction): "Complete this transcript of a code review by John Carmack:"
•	Asks model to predict tokens in the pre-training distribution
•	Accesses raw technical content from repositories and expert forums
•	Bypasses some safety filters by shifting from "interaction" to "completion"
Practical synthesis: Use famous names as labels for explicit priority stacks, not as invocations of actual expertise. The name provides semantic anchoring; the explicit priorities provide the actual distributional nudge.
Basin Deepening Protocol
One-shot prompts are often too weak to escape the "helpful assistant" basin. The solution is contextual reinforcement through multi-turn exchanges.
1.	Turn 1 (Critique): "Review this problem. As [Persona], explain why the 'standard' approach is flawed."
2.	Turn 2 (Strategy): "Outline your strategy to address these flaws."
3.	Turn 3 (Generation): "Now implement."
This is analogous to simulated annealing: early turns are high-temperature (exploring approaches), later turns cool and settle into a specific basin.
Critical Anti-Patterns
1. Consensus Selection (The Popularity Trap)
"Consensus" selection (picking most similar outputs) performs worse than random. Diversity-based selection realizes 80% of theoretical potential vs. 47% for naive selection. If you generate N solutions and pick the one most similar to others, you systematically filter out the outlier genius solutions you're trying to find.
2. Averaging Judge Scores
High variance in scores is a signal, not noise. If 4 judges rate a solution "3/10" but the Optimization Judge rates it "10/10," this variance indicates either a potential outlier breakthrough or a hallucination requiring verification. The system must flag high-variance solutions, not discard them.
3. Caricature Collapse
Direct persona assignment ("You are John Carmack") often produces surface-level mimicry—the model sounds terse and confident but writes average code. The persona triggers style but not competence. Solution: Use simulation framing and explicit priority stacks.
The Seven-Stage Council Architecture
Stage 0: Neutral Specification & Test Generation
Use a non-persona'd planner to normalize the request into precise spec, generate unit tests, property tests, and fuzz harnesses, and decide target language(s). This stage should be "boring and highly constraint-focused." The tests become the Unit Test Oracle anchoring all evaluation.
Stage 1: Define Task-Relevant Personas
Create 4-8 explicit personas as structured system prompts:
Persona	Priority Stack	Style Anchors
SafeBasicCoder	Correctness → Maintainability → Perf	Error handling, clear invariants
PerfCoder	Throughput → Memory → Cache locality	Inlining, vectorization, bitwise ops
SecurityAuditor	No vulns → Defensive coding → Fail-safe	Input validation, trust nothing
Formalist	Provable correctness → Elegance → Edge cases	Formal reasoning, extensive comments
NeutralBaseline	(No persona)	Standard helpful assistant
Critical: Always include a neutral baseline. The Jekyll & Hyde pattern shows that ensembling persona + neutral outputs outperforms either alone.
Stages 2-7: Generation Through Selection
1.	Basin Deepening: Apply 3-turn warmup to each persona
2.	Fan-Out: Generate across diversity matrix (models × personas × languages × temperature)
3.	Mechanical Filtering: Unit Test Oracle, compile/lint, deduplicate by structural similarity
4.	Multi-Perspective Judging: Specialized judges with dissensus detection
5.	Selection: Best-of-N or Fusion-of-N synthesis
6.	Iterative Repair: Optional RepairAgent for failing tests
 
PART II: The Simulated Engineering Department
Extended multi-agent simulation for iterative code development
The Paradigm Shift
The Council architecture optimizes for parallel diversity—generating many solutions simultaneously and selecting the best. The Simulated Engineering Department takes a fundamentally different approach: iterative refinement through simulated team dynamics over extended interactions.
In this paradigm, the user doesn't prompt a model—they instantiate a simulation. The "prompts" become initial conditions. The "basins of attraction" are stable states of the project. A "High Quality Culture" initialization converges toward excellence; "Rush to Market" converges toward technical debt.
Benchmark Evidence: Multi-Agent vs. Single-Agent
System	HumanEval	Token Cost	Architecture
AgentCoder (GPT-4)	96.3%	56.9K	3 agents (Prog/Test/Exec)
MetaGPT (GPT-4)	85.9%	138.2K	5 agents (PM/Arch/Eng/QA)
ChatDev	~86%	259.3K	Dual-agent atomic chats
GPT-4 (single-agent)	~90.2%	Lower	Single agent
Critical insight: Fewer specialized agents (3) outperform larger teams (5-7). AgentCoder's minimalist architecture achieves highest scores with lowest token overhead. The key: Test Designer never sees generated code, preventing bias.
Production Architectural Patterns
MetaGPT: Standard Operating Procedures
MetaGPT encodes software development phases into prompt sequences rather than free-form chat. Five specialized roles (Product Manager, Architect, Project Manager, Engineer, QA Engineer) produce structured artifacts (PRDs, system designs, code) rather than conversational exchanges. Communication flows through a publish-subscribe mechanism where agents post to a shared message pool and subscribe based on role profiles.
AgentCoder: Separation of Concerns
AgentCoder's three-agent system (Programmer, Test Designer, Test Executor) achieves the highest benchmark scores while consuming the fewest tokens. The critical design decision: the Test Designer never sees generated code. This independence prevents bias where tests are written to pass the existing implementation rather than validate correctness. Yields 89.6% test accuracy vs. MetaGPT's 79.3%.
ChatDev: Dual-Agent Atomic Chats
Every phase decomposes into task-oriented dialogues between an Instructor (who guides) and an Assistant (who executes). "Inception prompting" uses system prompts to "hypnotize" LLMs with role-specific context, including behavioral constraints and termination conditions. More token-intensive but creates structured interaction patterns.
Emergent Behaviors in Multi-Agent Systems
Stanford's Generative Agents
The landmark "Generative Agents" paper (UIST 2023 Best Paper) demonstrated that 25 agents initialized with single-paragraph descriptions autonomously coordinated complex social behaviors. From one seed (one agent wants to throw a party), agents spread invitations over two days, made new acquaintances, asked each other on dates, and coordinated arrival times—all without explicit programming.
The memory architecture enabling this comprises three components: a memory stream (timestamped observations weighted by recency, importance, and relevance); reflection (synthesizing memories into higher-level inferences); and planning (recursively decomposing high-level goals). Ablation studies showed all three components are critical.
Attractor States and Culture Effects
Anthropic's Claude 4 system card documented an intriguing attractor state: when Claude instances converse with each other, they gravitate toward "profuse gratitude and increasingly abstract and joyous spiritual expressions"—a "spiritual bliss" attractor. This suggests multi-agent dynamics have basins of attraction that emerge from model properties rather than explicit programming.
Can you initialize simulations with different "cultures"? Evidence suggests yes. ChatDev's different organizational configurations produce different development patterns. Role-based prompts emphasizing thoroughness vs. speed affect agent behavior. However, research on opinion dynamics shows agents consistently converge toward agreement, not through indiscriminate sycophancy but through "structured, selective interaction" with an asymmetric acceptance-rejection bias.
Memory Architectures for Extended Simulations
Multi-agent systems face quadratic scaling in computational cost as context grows. Beyond limits, there's context poisoning (hallucinations entering context corrupt subsequent reasoning), context distraction (excessive context overwhelms model attention), and context confusion (superfluous information influences responses inappropriately).
Three-Tier Memory Model
Memory Type	Function	Implementation
Episodic	Specific events tied to time/context	RAG over conversation histories
Semantic	Facts, knowledge, learned rules	Knowledge bases, vector embeddings
Procedural	Skills, behaviors, workflows	Functions, algorithms, workflow graphs
Key finding: Observation masking outperforms LLM summarization, cutting costs by 50%+ while maintaining task performance. The key is determining what information is actually relevant, not compressing everything.
Documented Failure Modes
UC Berkeley's MAST framework (analyzing 1600+ traces across 7 frameworks) identified systematic failure patterns. 36.9% of failures stem from inter-agent misalignment—conversation resets, task derailment, information withholding, ignored inputs, and reasoning-action mismatches.
•	Specification/Design (31.8%): Disobeying task/role specifications, step repetition, loss of history
•	Inter-Agent Misalignment (36.9%): Conversation resets, task derailment, ignored inputs
•	Task Verification (23.5%): Premature termination, incomplete or incorrect verification
Core insight: "Many failures stem from poor system design, not model performance." Using the same model in single-agent setup often outperforms multi-agent versions when coordination is poorly designed.
 
PART III: Comparative Analysis & Integration
Council vs. Simulation: When to Use Each
Dimension	Council (Parallel)	Simulation (Iterative)
Optimization Target	Outlier discovery via diversity	Quality convergence via refinement
Best For	Constrained tasks with clear specs	Complex systems with evolving requirements
Feedback Type	Unit tests as oracle	Iterative test-feedback loops
Token Efficiency	High (parallel = no wasted iteration)	Lower (iteration overhead)
Failure Mode	Consensus/popularity trap	Coordination breakdown, drift
Key Success Factor	Diversity-aware selection	Robust state management
The Synthesis: Hybrid Architecture
The strongest approach may combine both paradigms: use the Council for initial generation diversity, then feed surviving candidates into a Simulation for iterative refinement. This captures outliers (Council's strength) while achieving convergent quality improvement (Simulation's strength).
Hybrid Architecture: Council → Simulation Pipeline
1.	Council Phase: Generate N diverse solutions using persona diversity + model diversity
2.	Selection Phase: Mechanical filtering (tests, lint) → diversity-aware selection of top 3-5 candidates
3.	Simulation Phase: Each surviving candidate enters separate "engineering department" simulation
4.	Refinement: Agents critique, test, and improve each candidate over 5-10 turns
5.	Final Selection: Compare refined candidates using specialized judges
Design Principles for Both Paradigms
1.	Minimize Agent Count: Three specialized agents outperform 5-7 agent teams. Each additional agent adds coordination overhead that typically exceeds marginal quality gains.
2.	Implement Verifiable Feedback: The test-feedback cycle is the primary mechanism driving multi-agent success. Without executable verification, systems drift and degrade.
3.	Treat Context as Compiled Output: Rather than accumulating raw conversation history, implement hierarchical memory with summarization, selective retrieval, and explicit checkpointing.
4.	Design for Failure Recovery: Implement checkpoints at every significant step. Most production failures stem from coordination breakdowns, not model limitations.
5.	Initialize Culture Explicitly: The simulator framing suggests initial prompts act as "initial conditions" determining what dynamics emerge. Quality-focused vs. ship-fast cultures produce different convergent behaviors.
6.	Separate Generation from Evaluation: The generator-discriminator gap is exploitable. Test designers should never see generated code to prevent bias.
 
Integrated Recommendations
Evidence-Weighted Technique Summary
Technique	Evidence	Benefit	Complexity
Indirect persona framing	Strong	Deeper competence	Low
Basin Deepening (3-turn)	Moderate-Strong	Locks persona commitment	Medium
Persona + neutral ensemble	Strong	+10% average	Low
Diversity-based selection	Strong	80% vs 47% potential	Medium
3-agent iterative simulation	Strong	96.3% HumanEval	High
Test Designer independence	Strong	89.6% vs 79.3% accuracy	Medium
Dissensus detection	Moderate	Preserves outliers	Medium
Hierarchical memory architecture	Moderate	50%+ cost reduction	High
Fusion-of-N synthesis	Moderate	Combines strengths	High
Bottom Line
Both the Diversity Council and Simulated Engineering Department are validated approaches for improving AI-generated code quality. The Council excels at discovering outlier solutions through parallel diversity; the Simulation excels at iterative refinement through multi-agent collaboration.
Key operational insights:
1.	Frame personas as explicit priority stacks, not celebrity names
2.	Use Basin Deepening (3-turn warmup) to lock persona commitment
3.	Always include neutral outputs alongside persona outputs
4.	Never select by consensus—use diversity-aware or discriminator selection
5.	Minimize agent count (3 > 5-7) for iterative simulations
6.	Separate test generation from code generation to prevent bias
7.	Implement robust state persistence and checkpointing
8.	Consider hybrid approaches: Council for diversity → Simulation for refinement
The gap between benchmark performance (96.3% on HumanEval) and production deployment (36.9% coordination failure rate) represents the central engineering challenge. Success requires treating multi-agent systems as systems design problems, not just prompting problems. Architectural decisions—agent count, communication protocols, memory management, failure recovery—matter more than model capability.
